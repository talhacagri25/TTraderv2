{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Initialize Binance Client\n",
    "client = Client(None, None)\n",
    "\n",
    "# Function to download and merge data\n",
    "def download_and_merge_data(symbol, interval, start_date, end_date):\n",
    "    # Download USDT Parity Data\n",
    "    usdt_symbol = symbol + \"USDT\"\n",
    "    usdt_data = client.get_historical_klines(usdt_symbol, interval, start_date, end_date)\n",
    "    usdt_df = pd.DataFrame(usdt_data, columns=headers)\n",
    "\n",
    "    # Download BTC Parity Data\n",
    "    btc_symbol = symbol + \"BTC\"\n",
    "    btc_data = client.get_historical_klines(btc_symbol, interval, start_date, end_date)\n",
    "    btc_df = pd.DataFrame(btc_data, columns=headers)\n",
    "\n",
    "    # Rename columns for BTC parity\n",
    "    btc_df.rename(columns={\n",
    "        'Open': 'OpenBTC', \n",
    "        'High': 'HighBTC', \n",
    "        'Low': 'LowBTC', \n",
    "        'Close': 'CloseBTC', \n",
    "        'Volume': 'VolumeBTC'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Merge on Open Time\n",
    "    merged_data = usdt_df.merge(btc_df, on='Open Time', suffixes=('', '_BTC'))\n",
    "    return merged_data.drop(columns=[\"QAV\", \"NAT\", \"TBBAV\", \"TBQAV\", \"Ignore\"])\n",
    "\n",
    "# Function to calculate RSI\n",
    "def calculate_rsi(data, window=14):\n",
    "    delta = data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(features, labels, sequence_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(features) - sequence_length):\n",
    "        xs.append(features[i:(i + sequence_length)])\n",
    "        ys.append(labels[i])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Parameters\n",
    "interval = Client.KLINE_INTERVAL_5MINUTE\n",
    "start_date = \"1 January 2023\"\n",
    "end_date = \"8 December 2023\"\n",
    "headers = [\"Open Time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Close Time\", \"QAV\", \"NAT\", \"TBBAV\", \"TBQAV\", \"Ignore\"]\n",
    "sequence_length = 10\n",
    "coins = [\"ETH\"]  # List of coins\n",
    "\n",
    "# Initialize scalers\n",
    "feature_scaler = StandardScaler()\n",
    "label_scaler = StandardScaler()\n",
    "\n",
    "# Process each coin\n",
    "all_features, all_labels = [], []\n",
    "for coin in coins:\n",
    "    coin_df = download_and_merge_data(coin, interval, start_date, end_date)\n",
    "\n",
    "    # Convert timestamps and drop unnecessary columns\n",
    "    coin_df['Open Time'] = pd.to_datetime(coin_df['Open Time'], unit='ms')\n",
    "    coin_df['Close Time'] = pd.to_datetime(coin_df['Close Time'], unit='ms')\n",
    "    coin_df = coin_df.drop(columns=[\"QAV_BTC\", \"NAT_BTC\", \"TBBAV_BTC\", \"TBQAV_BTC\", \"Ignore_BTC\",\"Close Time_BTC\"])\n",
    "\n",
    "    # Convert numerical columns to numeric type\n",
    "    for col in [col for col in coin_df.columns if col not in ['Open Time', 'Close Time']]:\n",
    "        coin_df[col] = pd.to_numeric(coin_df[col])\n",
    "\n",
    "    # Replace any infinite values with NaN and drop rows with NaN values\n",
    "    coin_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    coin_df.dropna(inplace=True)\n",
    "\n",
    "    # Calculate technical indicators\n",
    "    coin_df['EMA55'] = coin_df['Close'].ewm(span=55, adjust=False).mean()\n",
    "    coin_df['EMA5'] = coin_df['Close'].ewm(span=5, adjust=False).mean()\n",
    "    coin_df['EMA10'] = coin_df['Close'].ewm(span=10, adjust=False).mean()\n",
    "    coin_df['SMA9'] = coin_df['Close'].rolling(window=9).mean()\n",
    "    coin_df['Above_SMA9'] = (coin_df['Close'] > coin_df['SMA9']).astype(int)\n",
    "    coin_df['Above_EMA55'] = (coin_df['Close'] > coin_df['EMA55']).astype(int)\n",
    "    window_size = 10\n",
    "    coin_df['Volume_Price_Correlation'] = coin_df['Volume'].rolling(window=window_size).corr(coin_df['Close'])\n",
    "    coin_df['RSI'] = calculate_rsi(coin_df)\n",
    "    coin_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    coin_df.dropna(inplace=True)\n",
    "\n",
    "    # Prepare labels for sequence creation\n",
    "    labels_high = coin_df['High'].shift(-sequence_length)\n",
    "    labels_low = coin_df['Low'].shift(-sequence_length)\n",
    "\n",
    "    # Combine labels\n",
    "    labels = np.column_stack([labels_high, labels_low])\n",
    "\n",
    "    # Dynamic Feature Selection and Normalization\n",
    "    features = coin_df.drop(columns=['Open Time', 'Close Time'])\n",
    "    scaled_features = feature_scaler.fit_transform(features)\n",
    "\n",
    "    # Combine features and labels for each coin\n",
    "    all_features.append(scaled_features)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Combine all features and labels from different coins\n",
    "all_features = np.concatenate(all_features, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Handle NaN in labels\n",
    "mask = ~np.isnan(all_labels).any(axis=1) \n",
    "all_features = all_features[mask]\n",
    "all_labels = all_labels[mask]\n",
    "\n",
    "# Scale labels\n",
    "y_scaled = label_scaler.fit_transform(labels.reshape(-1, 2))\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(len(all_features) * 0.8)\n",
    "X_train_unscaled, X_test_unscaled = all_features[:train_size], all_features[train_size:]\n",
    "y_train, y_test = y_scaled[:train_size], y_scaled[train_size:]\n",
    "\n",
    "# Create sequences for training and testing sets\n",
    "X_train, y_train_seq = create_sequences(X_train_unscaled, y_train, sequence_length)\n",
    "X_test, y_test_seq = create_sequences(X_test_unscaled, y_test, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "y_train_seq = y_train_seq[:len(X_train)]\n",
    "y_test_seq = y_test_seq[:len(X_test)]\n",
    "\n",
    "indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Shuffle the training data using the generated indices\n",
    "X_train_shuffled = X_train[indices]\n",
    "y_train_shuffled = y_train_seq[indices]\n",
    "log_dir = \"logs/fit1/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def mase(y_true, y_pred):\n",
    "    # Calculate the MAE for the predictions\n",
    "    mae = K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "    # Calculate the MAE of the naive forecast (naive forecast = last observed value)\n",
    "    mae_naive = K.mean(K.abs(y_true[1:] - y_true[:-1]))\n",
    "\n",
    "    # Prevent division by zero\n",
    "    mae_naive = K.maximum(mae_naive, K.epsilon())\n",
    "\n",
    "    # Calculate MASE\n",
    "    return mae / mae_naive\n",
    "\n",
    "dropout_rate = 0.2\n",
    "l1_reg = 1e-5\n",
    "l2_reg = 1e-4\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer with L1/L2 regularization\n",
    "model.add(LSTM(units=256, return_sequences=True,\n",
    "               activation='swish', recurrent_activation='sigmoid'))\n",
    "#model.add(Dropout(dropout_rate))\n",
    "# Second LSTM layer\n",
    "model.add(LSTM(units=128, return_sequences=False, \n",
    "               activation='swish', recurrent_activation='sigmoid'))\n",
    "\n",
    "# Dense layer for output\n",
    "model.add(Dense(2))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[mase])\n",
    "joblib.dump(feature_scaler, 'feature_scaler.pkl')\n",
    "joblib.dump(label_scaler, 'label_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'model5.keras',  # Path where to save the model\n",
    "    monitor='val_mase',  # Monitor the validation loss\n",
    "    verbose=1,  # Verbosity mode\n",
    "    save_best_only=True,  # Save only when the monitored metric has improved\n",
    "    mode='min'  # The monitoring mode ('min' for minimization)\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_shuffled, y_train_shuffled, \n",
    "    epochs=70, \n",
    "    validation_data=(X_test, y_test_seq),\n",
    "    callbacks=[tensorboard_callback, checkpoint_callback]  # Include checkpoint_callback here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "with custom_object_scope({'mase': mase}):\n",
    "    model = load_model('model5.keras')\n",
    "feature_scaler = joblib.load('feature_scaler.pkl')\n",
    "label_scaler = joblib.load('label_scaler.pkl')\n",
    "pred = model.predict(X_test)\n",
    "print(\"Shape of pred:\", pred.shape)\n",
    "# Reshape predictions to 2D if necessary\n",
    "if pred.ndim == 2 and pred.shape[1] == 1:\n",
    "    # If the predictions are of shape (389, 1), this indicates an issue with the model's output layer\n",
    "    raise ValueError(\"Model is not outputting predictions for both 'High' and 'Low' prices.\")\n",
    "\n",
    "\n",
    "# If your target variable was scaled, inverse transform the predictions\n",
    "inverse_pred = label_scaler.inverse_transform(pred)\n",
    "\n",
    "predicted_high = inverse_pred[:, 0]\n",
    "predicted_low = inverse_pred[:, 1]\n",
    "\n",
    "# Reshape y_test_seq to 2D for inverse scaling and metrics\n",
    "y_test_seq_reshaped = y_test_seq.reshape(-1, 2)\n",
    "\n",
    "# Inverse transform the actual target values in the test set\n",
    "inverse_test_y = label_scaler.inverse_transform(y_test_seq_reshaped)\n",
    "\n",
    "# Calculate Normalized RMSE (on scaled data)\n",
    "norm_rmse = np.sqrt(mean_squared_error(y_test_seq, pred))\n",
    "print('Normalized RMSE:', norm_rmse)\n",
    "\n",
    "# Calculate Absolute RMSE (on original scale data)\n",
    "abs_rmse = np.sqrt(mean_squared_error(inverse_test_y, inverse_pred))\n",
    "print('Absolute RMSE:', abs_rmse)\n",
    "\n",
    "# Define Mean Absolute Percentage Error function\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Calculate Normalized MAPE\n",
    "norm_mape = mean_absolute_percentage_error(y_test_seq, pred)\n",
    "print('Normalized MAPE:', norm_mape, '%')\n",
    "\n",
    "# Calculate Absolute MAPE\n",
    "abs_mape = mean_absolute_percentage_error(inverse_test_y, inverse_pred)\n",
    "print('Absolute MAPE:', abs_mape, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming inverse_test_y[:, 0] is the true high and inverse_test_y[:, 1] is the true low\n",
    "true_high = inverse_test_y[:, 0]\n",
    "true_low = inverse_test_y[:, 1]\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(true_high, label='Actual High', color='blue')\n",
    "plt.plot(predicted_high, label='Predicted High', color='red', alpha=0.7)\n",
    "plt.plot(true_low, label='Actual Low', color='green')\n",
    "plt.plot(predicted_low, label='Predicted Low', color='orange', alpha=0.7)\n",
    "plt.title('Comparison of Actual and Predicted High/Low Prices')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pytz\n",
    "import time\n",
    "from binance.client import Client\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_tuner import HyperModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pytz\n",
    "import time\n",
    "from binance.client import Client\n",
    "\n",
    "client = Client(None, None)\n",
    "\n",
    "headers = [\"Open Time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Close Time\", \"QAV\", \"NAT\", \"TBBAV\", \"TBQAV\", \"Ignore\"]\n",
    "\n",
    "def mase(y_true, y_pred):\n",
    "    # Calculate the MAE for the predictions\n",
    "    mae = K.mean(K.abs(y_pred - y_true))\n",
    "\n",
    "    # Calculate the MAE of the naive forecast (naive forecast = last observed value)\n",
    "    mae_naive = K.mean(K.abs(y_true[1:] - y_true[:-1]))\n",
    "\n",
    "    # Prevent division by zero\n",
    "    mae_naive = K.maximum(mae_naive, K.epsilon())\n",
    "\n",
    "    # Calculate MASE\n",
    "    return mae / mae_naive\n",
    "\n",
    "def calculate_rsi(data, window=14):\n",
    "    delta = data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def download_and_merge_data(symbol, interval, start_date, end_date):\n",
    "    # Download USDT Parity Data\n",
    "    usdt_symbol = symbol + \"USDT\"\n",
    "    usdt_data = client.get_historical_klines(usdt_symbol, interval, limit=30)\n",
    "    usdt_df = pd.DataFrame(usdt_data, columns=headers)\n",
    "\n",
    "    # Download BTC Parity Data\n",
    "    btc_symbol = symbol + \"BTC\"\n",
    "    btc_data = client.get_historical_klines(btc_symbol, interval, limit=30)\n",
    "    btc_df = pd.DataFrame(btc_data, columns=headers)\n",
    "\n",
    "    # Rename columns for BTC parity\n",
    "    btc_df.rename(columns={\n",
    "        'Open': 'OpenBTC', \n",
    "        'High': 'HighBTC', \n",
    "        'Low': 'LowBTC', \n",
    "        'Close': 'CloseBTC', \n",
    "        'Volume': 'VolumeBTC'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Merge on Open Time\n",
    "    merged_data = usdt_df.merge(btc_df, on='Open Time', suffixes=('', '_BTC'))\n",
    "    return merged_data.drop(columns=[\"QAV\", \"NAT\", \"TBBAV\", \"TBQAV\", \"Ignore\"])\n",
    "\n",
    "def preprocess_realtime_data(data):\n",
    "    # Assuming the data includes both USDT and BTC parity data merged\n",
    "\n",
    "    # Convert timestamps\n",
    "    data['Open Time'] = pd.to_datetime(data['Open Time'], errors='coerce')\n",
    "    data['Close Time'] = pd.to_datetime(data['Close Time'], errors='coerce')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    data = data.drop(columns=[\"QAV_BTC\", \"NAT_BTC\", \"TBBAV_BTC\", \"TBQAV_BTC\", \"Ignore_BTC\", \"Close Time_BTC\"])\n",
    "\n",
    "    # Convert numerical columns to numeric type\n",
    "    for col in [col for col in data.columns if col not in ['Open Time', 'Close Time']]:\n",
    "        data[col] = pd.to_numeric(data[col])\n",
    "\n",
    "    # Replace any infinite values with NaN and drop rows with NaN values\n",
    "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Calculate technical indicators as done during training\n",
    "    data['EMA55'] = data['Close'].ewm(span=55, adjust=False).mean()\n",
    "    data['EMA5'] = data['Close'].ewm(span=5, adjust=False).mean()\n",
    "    data['EMA10'] = data['Close'].ewm(span=10, adjust=False).mean()\n",
    "    data['SMA9'] = data['Close'].rolling(window=9).mean()\n",
    "    data['Above_SMA9'] = (data['Close'] > data['SMA9']).astype(int)\n",
    "    data['Above_EMA55'] = (data['Close'] > data['EMA55']).astype(int)\n",
    "    window_size = 10\n",
    "    data['Volume_Price_Correlation'] = data['Volume'].rolling(window=window_size).corr(data['Close'])\n",
    "    data['RSI'] = calculate_rsi(data)\n",
    "\n",
    "    # Select and return the relevant features used in your model\n",
    "    return data[['Open','High','Low','Close','Volume','OpenBTC','HighBTC','LowBTC','CloseBTC','VolumeBTC','EMA55','EMA5','EMA10','SMA9','Above_SMA9','Above_EMA55','Volume_Price_Correlation','RSI']]\n",
    "\n",
    "\n",
    "def fetch_latest_candles(symbol, interval, start_date='1 day ago UTC', end_date='now UTC'):\n",
    "    return download_and_merge_data(symbol, interval, start_date, end_date)\n",
    "\n",
    "# Initialize data_queue with the latest 14 candlesticks\n",
    "coin = \"ETH\"\n",
    "interval = Client.KLINE_INTERVAL_5MINUTE\n",
    "start_date = \"1 day ago UTC\"\n",
    "end_date = \"now UTC\"\n",
    "\n",
    "# Fetch initial data to start the data_queue\n",
    "initial_data = fetch_latest_candles(coin, interval, start_date, end_date)\n",
    "data_queue = initial_data.tail(30)\n",
    "\n",
    "with custom_object_scope({'mase': mase}):\n",
    "    model = load_model('model5.keras')\n",
    "feature_scaler = joblib.load('feature_scaler.pkl')\n",
    "label_scaler = joblib.load('label_scaler.pkl')\n",
    "\n",
    "# Timezone setup\n",
    "tz = pytz.timezone('Turkey')\n",
    "\n",
    "# Initialize variables for tracking BUY/SELL and P/L\n",
    "predictions = [0, 0]\n",
    "temp = []\n",
    "p_and_l = 0\n",
    "_df = []\n",
    "p = None\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    current_time = dt.datetime.now(tz=tz)\n",
    "    if current_time.minute % 5 == 0 and current_time.second == 0:\n",
    "        try:\n",
    "            # Fetch the latest candlestick data\n",
    "            latest_candles = fetch_latest_candles(coin, interval, start_date, end_date)\n",
    "            new_candle = latest_candles.iloc[-1]  # Get the most recent candle\n",
    "\n",
    "            data_queue = pd.concat([data_queue, pd.DataFrame([new_candle])]).tail(30)\n",
    "\n",
    "            # Process and predict\n",
    "            processed_data = preprocess_realtime_data(data_queue)\n",
    "            scaled_data = feature_scaler.transform(processed_data)\n",
    "            data_sequence = scaled_data[-10:]  # Assuming your model expects sequences of length 10\n",
    "            data_sequence = np.reshape(data_sequence, (1, 10, -1))\n",
    "            pred = model.predict(data_sequence)\n",
    "            inv_pred = np.rint(label_scaler.inverse_transform(pred))\n",
    "            print(\"Prediction -> \", inv_pred, '\\n')\n",
    "\n",
    "            predictions.pop(0)\n",
    "            predictions.append(inv_pred)\n",
    "\n",
    "            _p = p\n",
    "\n",
    "            # Update the current price based on the new candle\n",
    "            price = float(new_candle['Close'])\n",
    "\n",
    "            print(\"Prediction is: \", predictions[1] - predictions[0])\n",
    "            if len(predictions) > 1 and (predictions[1] - predictions[0] > 0):\n",
    "                p = 1\n",
    "                _ = 'BUY'\n",
    "            else:\n",
    "                p = 0\n",
    "                _ = 'SELL'\n",
    "\n",
    "            temp.append(price)\n",
    "            print(\"temp -> \", temp)\n",
    "\n",
    "            if _p is not None and _p != p:\n",
    "                # Convert string values in 'temp' to floats before subtraction\n",
    "                if p == 1:\n",
    "                    p_and_l += float(temp[0]) - float(temp[-1])\n",
    "                elif p == 0:\n",
    "                    p_and_l += float(temp[-1]) - float(temp[0])\n",
    "\n",
    "                temp = []\n",
    "                p = None\n",
    "\n",
    "            _df.append([_, temp, p_and_l, dt.datetime.now(tz=tz)])\n",
    "\n",
    "            count += 1\n",
    "        except ValueError as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "        time.sleep(60)  # Wait to avoid multiple triggers within the same minute\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
